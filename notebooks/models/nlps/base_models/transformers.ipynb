{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**🔹 Transformers: A Detailed Explanation**\n",
    "===========================================\n",
    "\n",
    "Transformers are the foundation of modern **AI-driven NLP models**, including **GPT**, **BERT**, **T5**, and **LLMs** used in chatbots, machine translation, and text generation. They have **revolutionized NLP**, replacing older approaches like **RNNs (Recurrent Neural Networks) and LSTMs (Long Short-Term Memory networks).**\n",
    "\n",
    "Let’s **break down transformers** step by step **without too much math** but with **clear concepts and examples**. 🚀\n",
    "\n",
    "**📌 Why Do We Need Transformers?**\n",
    "-----------------------------------\n",
    "\n",
    "Before Transformers, models like **RNNs and LSTMs** were used for text processing, but they had major **limitations**:\n",
    "\n",
    "❌ **Sequential processing:** RNNs process words one by one, making them **slow**.\n",
    "\n",
    "❌ **Long-term memory loss:** LSTMs help but still **struggle with very long sentences**.\n",
    "\n",
    "❌ **Limited parallelism:** GPUs can’t efficiently process RNNs because they process sequentially.\n",
    "\n",
    "🔹 **Transformers solve these issues** using a concept called **self-attention**, allowing them to process entire sentences **in parallel** and **capture long-range dependencies effectively**.\n",
    "\n",
    "**🔹 How Transformers Work: A High-Level View**\n",
    "-----------------------------------------------\n",
    "\n",
    "A **transformer model** takes an input sequence (like a sentence) and processes it using multiple layers of **self-attention and feed-forward networks** to generate meaningful output.\n",
    "\n",
    "The architecture consists of two main parts:\n",
    "\n",
    "### **1️⃣ Encoder** – Reads and processes input (e.g., a sentence in English).\n",
    "\n",
    "### **2️⃣ Decoder** – Generates the output (e.g., translates the sentence into French).\n",
    "\n",
    "📌 **For models like BERT, only the encoder is used (for text understanding).**\n",
    "\n",
    "📌 **For models like GPT, only the decoder is used (for text generation).**\n",
    "\n",
    "📌 **For translation models like T5, both encoder and decoder are used.**\n",
    "\n",
    "**🔹 Step-by-Step Breakdown of Transformers**\n",
    "---------------------------------------------\n",
    "\n",
    "Let's walk through the **key components** that make transformers work:\n",
    "\n",
    "### **1️⃣ Tokenization (Breaking Text into Pieces)**\n",
    "\n",
    "Before feeding text into a transformer, it needs to be **tokenized** (split into words or subwords).For example, the sentence:👉 **\"The cat sat on the mat.\"**Might be tokenized as:\n",
    "\n",
    "```python\n",
    "[\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\", \".\"]\n",
    "```\n",
    "\n",
    "Advanced tokenization (used in BERT/GPT) breaks it into **subwords**:\n",
    "\n",
    "```python\n",
    "[\"The\", \"cat\", \"sat\", \"on\", \"the\", \"ma\", \"t\", \".\"]\n",
    "```\n",
    "\n",
    "### **2️⃣ Positional Encoding (Adding Word Order)**\n",
    "\n",
    "Unlike RNNs, transformers **don’t process words sequentially**.Since word order is **important**, transformers add **positional encodings** (numerical representations) to retain word order.\n",
    "\n",
    "For example:\n",
    "\n",
    "| Token  | Position Encoding |\n",
    "|--------|------------------|\n",
    "| The    | 0.21            |\n",
    "| cat    | 0.44            |\n",
    "| sat    | 0.62            |\n",
    "| on     | 0.81            |\n",
    "| the    | 0.95            |\n",
    "| mat    | 1.12            |\n",
    "\n",
    "This helps the model **know the position** of words.\n",
    "\n",
    "### **3️⃣ Self-Attention (Understanding Word Relationships)**\n",
    "\n",
    "This is the most important concept!\n",
    "\n",
    "💡 **What is self-attention?\n",
    "**It allows the transformer to **focus on different words in a sentence depending on context.**\n",
    "\n",
    "**Example:**Consider two sentences:\n",
    "1️⃣ **\"I went to the bank to deposit money.\"**\n",
    "2️⃣ **\"I sat by the bank of the river.\"**\n",
    "\n",
    "The word **\"bank\"** has different meanings in each sentence.\n",
    "Self-attention helps the model **look at surrounding words** to understand the meaning of \"bank\" correctly.\n",
    "\n",
    "#### **How Self-Attention Works (Conceptually)**\n",
    "\n",
    "*   Each word gets **three vectors**:\n",
    "    \n",
    "    *   **Query (Q)** – What is this word looking for?\n",
    "        \n",
    "    *   **Key (K)** – How important is this word for others?\n",
    "        \n",
    "    *   **Value (V)** – What information does this word carry?\n",
    "        \n",
    "*   The model **compares every word to every other word** and assigns **attention scores**.\n",
    "    \n",
    "*   Words with **higher scores** are more relevant.\n",
    "    \n",
    "\n",
    "🔹 **Example (Attention Scores for \"bank\")**\n",
    "\n",
    "| Word    | \"bank\" in Sentence 1 | \"bank\" in Sentence 2 |\n",
    "|---------|----------------------|----------------------|\n",
    "| I       | 0.1                  | 0.1                  |\n",
    "| went    | 0.2                  | 0.2                  |\n",
    "| to      | 0.3                  | 0.3                  |\n",
    "| the     | 0.5                  | 0.5                  |\n",
    "| bank    | 1.0                  | 1.0                  |\n",
    "| deposit | 0.9                  | 0.1                  |\n",
    "| river   | 0.1                  | 0.9                  |\n",
    "\n",
    "Here, **\"deposit\" has a high score in Sentence 1**, meaning \"bank\" refers to a financial institution.In Sentence 2, **\"river\" has a high score**, so \"bank\" refers to a riverbank.\n",
    "\n",
    "This is how transformers **understand context better than RNNs**.\n",
    "\n",
    "### **4️⃣ Multi-Head Attention (Learning Multiple Contexts)**\n",
    "\n",
    "Instead of looking at just **one relationship at a time**, transformers use **multiple attention heads** to understand **different aspects of meaning simultaneously**.\n",
    "\n",
    "For example, in **machine translation**:\n",
    "\n",
    "*   **One head** might focus on grammar.\n",
    "    \n",
    "*   **Another head** might focus on word meaning.\n",
    "    \n",
    "*   **Another head** might focus on sentence structure.\n",
    "    \n",
    "\n",
    "This makes transformers **powerful and highly context-aware**.\n",
    "\n",
    "### **5️⃣ Feed-Forward Neural Network**\n",
    "\n",
    "After self-attention, each word passes through a **feed-forward neural network** that refines the understanding.These networks help **add more complex reasoning** before making predictions.\n",
    "\n",
    "### **6️⃣ Layer Normalization & Residual Connections**\n",
    "\n",
    "To make training stable and efficient, transformers use: \n",
    "\n",
    "✅ **Residual connections** – Prevent information loss.\n",
    "\n",
    "✅ **Layer normalization** – Ensure numerical stability.\n",
    "\n",
    "**🔹 Example: How Transformers Work in Text Generation**\n",
    "--------------------------------------------------------\n",
    "\n",
    "Let’s say we train a transformer to predict the next word in a sentence.\n",
    "\n",
    "Input:👉 **\"The cat sat on the\"**Possible outputs:\n",
    "\n",
    "🔹 \"mat\" (80% confidence)\n",
    "\n",
    "🔹 \"sofa\" (10% confidence)\n",
    "\n",
    "🔹 \"chair\" (5% confidence)\n",
    "\n",
    "🔹 \"floor\" (5% confidence)\n",
    "\n",
    "The model **assigns probabilities** to each possible next word and picks the most likely one.\n",
    "\n",
    "**🔹 Comparison: Transformers vs. RNNs/LSTMs**\n",
    "----------------------------------------------\n",
    "\n",
    "| Feature              | RNN/LSTM                  | Transformer              |\n",
    "|----------------------|--------------------------|--------------------------|\n",
    "| **Processing**       | Sequential (word-by-word) | Parallel (Fast)          |\n",
    "| **Long-Term Memory** | Struggles with long texts | Handles long-range dependencies |\n",
    "| **Training Speed**   | Slow                      | Fast (processes entire text at once) |\n",
    "| **Context Understanding** | Limited             | Strong (Self-Attention)  |\n",
    "| **Scalability**      | Hard to scale             | Easily scales with GPUs  |\n",
    "\n",
    "\n",
    "**🔹 Real-World Applications of Transformers**\n",
    "----------------------------------------------\n",
    "\n",
    "🚀 **GPT (ChatGPT, GPT-4)** – Conversational AI.\n",
    "\n",
    "🚀 **BERT** – Google Search, sentiment analysis.\n",
    "\n",
    "🚀 **T5 (Text-to-Text Transfer Transformer)** – Summarization, translation.\n",
    "\n",
    "🚀 **DALL·E** – Image generation from text prompts.\n",
    "\n",
    "🚀 **Whisper** – Automatic speech recognition (ASR).\n",
    "\n",
    "**🔹 Comparison: GPT and BERT**\n",
    "----------------------------------------------\n",
    "\n",
    "| Feature                      | Generative Pre-trained Transformer (GPT) | Bidirectional Encoder Representations from Transformers (BERT) |\n",
    "|------------------------------|------------------------------------------|---------------------------------------------------------------|\n",
    "| **Architecture**             | Decoder-only Transformer                 | Encoder-only Transformer                                      |\n",
    "| **Training Approach**        | Autoregressive (predicts next token)     | Autoencoding (masked language model)                          |\n",
    "| **Directionality**           | Unidirectional (left-to-right)           | Bidirectional (both left and right context)                   |\n",
    "| **Primary Use Case**         | Text generation, dialogue, content creation | Text understanding, classification, sentiment analysis       |\n",
    "| **Example Tasks**            | Chatbots, story writing, code generation  | Q&A, Named Entity Recognition, Sentiment Analysis             |\n",
    "| **Input Type**               | Processes input sequentially (causal)    | Processes entire input simultaneously                         |\n",
    "| **Pretraining Objective**    | Next-word prediction                     | Masked word prediction + Next sentence prediction             |\n",
    "| **Context Awareness**        | Limited by past tokens only              | Full sentence-level understanding                             |\n",
    "| **Computational Cost**       | High (due to sequential generation)      | Lower (processes tokens in parallel)                          |\n",
    "| **Fine-tuning Flexibility**  | Good for open-ended text generation      | Better for structured NLP tasks                              |\n",
    "| **Strengths**                | Generates coherent, fluent text          | Strong at understanding sentence semantics                   |\n",
    "| **Weaknesses**               | May hallucinate or generate incorrect info | Not good at free-text generation                             |\n",
    "| **Famous Models**            | GPT-3, GPT-4, ChatGPT                    | BERT, RoBERTa, DistilBERT                                    |\n",
    "| **Use in Industry**          | AI assistants, content creation, coding  | Search engines, customer support, text classification        |\n",
    "\n",
    "\n",
    "**🔹 Summary**\n",
    "--------------\n",
    "\n",
    "✅ Transformers use **self-attention** to understand context.\n",
    "\n",
    "✅ They **process entire text in parallel**, making them **fast and scalable**.\n",
    "\n",
    "✅ Used in **chatbots, search engines, translation, and AI text generation**.\n",
    "\n",
    "✅ **Outperform RNNs and LSTMs** in most NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-30 14:02:31.997657: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' floor', ' bed', ' couch', ' ground', ' edge']\n",
      "[' a', ' the', ' an', ' not', ' one']\n",
      "[' the', ' our', ' how', ' their', ' everything']\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel  # For loading the GPT-2 model and tokenizer\n",
    "import torch  # For tensor operations and model inference\n",
    "\n",
    "# -------------------------------\n",
    "# Load Pre-trained GPT-2 Model and Tokenizer\n",
    "# -------------------------------\n",
    "\n",
    "# Load the pre-trained GPT-2 tokenizer\n",
    "# The tokenizer converts input text into token IDs that the model can process.\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Load the pre-trained GPT-2 language model\n",
    "# The model generates text by predicting the next word based on the input context.\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model.eval()  # Set the model to evaluation mode (disables dropout, etc.)\n",
    "\n",
    "# -------------------------------\n",
    "# Prediction Function\n",
    "# -------------------------------\n",
    "\n",
    "def predict_next_word(text, top_k=5):\n",
    "    if not text.strip():\n",
    "        raise ValueError(\"Input text cannot be empty\")\n",
    "    \"\"\"\n",
    "    Predicts the next word(s) for a given input text using the GPT-2 model.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text for which the next word(s) are predicted.\n",
    "        top_k (int): The number of top probable next words to return (default is 5).\n",
    "\n",
    "    Returns:\n",
    "        list of str: A list of the top `top_k` predicted next words.\n",
    "\n",
    "    Steps:\n",
    "    1. Tokenize the input text using the GPT-2 tokenizer.\n",
    "    2. Pass the tokenized input to the GPT-2 model to get the logits (raw predictions).\n",
    "    3. Extract the logits for the last token in the sequence.\n",
    "    4. Apply the softmax function to convert logits into probabilities.\n",
    "    5. Use `torch.topk` to get the top `top_k` tokens with the highest probabilities.\n",
    "    6. Decode the token IDs back into words using the tokenizer.\n",
    "\n",
    "    Example:\n",
    "        Input: \"The cat sat on the\"\n",
    "        Output: [\"mat\", \"floor\", \"sofa\", \"chair\", \"bed\"]\n",
    "    \"\"\"\n",
    "    # Tokenize the input text and convert it into tensors\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    \n",
    "    # Perform inference with the GPT-2 model (no gradient computation needed)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Extract the logits for the last token in the sequence\n",
    "    logits = outputs.logits[:, -1, :]\n",
    "    \n",
    "    # Apply softmax to convert logits into probabilities\n",
    "    probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    \n",
    "    # Get the top `top_k` tokens with the highest probabilities\n",
    "    top_k_tokens = torch.topk(probabilities, top_k)\n",
    "    \n",
    "    # Decode the token IDs back into words\n",
    "    next_words = [tokenizer.decode([token]) for token in top_k_tokens.indices[0]]\n",
    "    \n",
    "    return next_words\n",
    "\n",
    "# -------------------------------\n",
    "# Example Predictions\n",
    "# -------------------------------\n",
    "\n",
    "# Predict the next word(s) for various input texts\n",
    "# The predictions are based on the context provided in the input text.\n",
    "print(predict_next_word(\"The cat sat on the\"))  # Example output: [\"mat\", \"floor\", \"sofa\", \"chair\", \"bed\"]\n",
    "print(predict_next_word(\"Deep learning is\"))  # Example output: [\"transforming\", \"revolutionizing\", \"advancing\", \"changing\", \"reshaping\"]\n",
    "print(predict_next_word(\"Transformers are revolutionizing\"))  # Example output: [\"AI\", \"technology\", \"NLP\", \"research\", \"science\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
