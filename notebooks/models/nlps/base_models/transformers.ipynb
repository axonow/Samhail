{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ğŸ”¹ Transformers: A Detailed Explanation**\n",
    "===========================================\n",
    "\n",
    "Transformers are the foundation of modernÂ **AI-driven NLP models**, includingÂ **GPT**,Â **BERT**,Â **T5**, andÂ **LLMs** used in chatbots, machine translation, and text generation. They haveÂ **revolutionized NLP**, replacing older approaches likeÂ **RNNs (Recurrent Neural Networks) and LSTMs (Long Short-Term Memory networks).**\n",
    "\n",
    "Letâ€™sÂ **break down transformers**Â step by stepÂ **without too much math**Â but withÂ **clear concepts and examples**. ğŸš€\n",
    "\n",
    "**ğŸ“Œ Why Do We Need Transformers?**\n",
    "-----------------------------------\n",
    "\n",
    "Before Transformers, models likeÂ **RNNs and LSTMs**Â were used for text processing, but they had majorÂ **limitations**:\n",
    "\n",
    "âŒÂ **Sequential processing:**Â RNNs process words one by one, making themÂ **slow**.\n",
    "\n",
    "âŒÂ **Long-term memory loss:**Â LSTMs help but stillÂ **struggle with very long sentences**.\n",
    "\n",
    "âŒÂ **Limited parallelism:**Â GPUs canâ€™t efficiently process RNNs because they process sequentially.\n",
    "\n",
    "ğŸ”¹Â **Transformers solve these issues**Â using a concept calledÂ **self-attention**, allowing them to process entire sentencesÂ **in parallel**Â andÂ **capture long-range dependencies effectively**.\n",
    "\n",
    "**ğŸ”¹ How Transformers Work: A High-Level View**\n",
    "-----------------------------------------------\n",
    "\n",
    "AÂ **transformer model**Â takes an input sequence (like a sentence) and processes it using multiple layers ofÂ **self-attention and feed-forward networks**Â to generate meaningful output.\n",
    "\n",
    "The architecture consists of two main parts:\n",
    "\n",
    "### **1ï¸âƒ£ Encoder**Â â€“ Reads and processes input (e.g., a sentence in English).\n",
    "\n",
    "### **2ï¸âƒ£ Decoder**Â â€“ Generates the output (e.g., translates the sentence into French).\n",
    "\n",
    "ğŸ“ŒÂ **For models like BERT, only the encoder is used (for text understanding).**\n",
    "\n",
    "ğŸ“ŒÂ **For models like GPT, only the decoder is used (for text generation).**\n",
    "\n",
    "ğŸ“ŒÂ **For translation models like T5, both encoder and decoder are used.**\n",
    "\n",
    "**ğŸ”¹ Step-by-Step Breakdown of Transformers**\n",
    "---------------------------------------------\n",
    "\n",
    "Let's walk through theÂ **key components**Â that make transformers work:\n",
    "\n",
    "### **1ï¸âƒ£ Tokenization (Breaking Text into Pieces)**\n",
    "\n",
    "Before feeding text into a transformer, it needs to beÂ **tokenized**Â (split into words or subwords).For example, the sentence:ğŸ‘‰Â **\"The cat sat on the mat.\"**Might be tokenized as:\n",
    "\n",
    "```python\n",
    "[\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\", \".\"]\n",
    "```\n",
    "\n",
    "Advanced tokenization (used in BERT/GPT) breaks it intoÂ **subwords**:\n",
    "\n",
    "```python\n",
    "[\"The\", \"cat\", \"sat\", \"on\", \"the\", \"ma\", \"t\", \".\"]\n",
    "```\n",
    "\n",
    "### **2ï¸âƒ£ Positional Encoding (Adding Word Order)**\n",
    "\n",
    "Unlike RNNs, transformersÂ **donâ€™t process words sequentially**.Since word order isÂ **important**, transformers addÂ **positional encodings**Â (numerical representations) to retain word order.\n",
    "\n",
    "For example:\n",
    "\n",
    "| Token  | Position Encoding |\n",
    "|--------|------------------|\n",
    "| The    | 0.21            |\n",
    "| cat    | 0.44            |\n",
    "| sat    | 0.62            |\n",
    "| on     | 0.81            |\n",
    "| the    | 0.95            |\n",
    "| mat    | 1.12            |\n",
    "\n",
    "This helps the modelÂ **know the position**Â of words.\n",
    "\n",
    "### **3ï¸âƒ£ Self-Attention (Understanding Word Relationships)**\n",
    "\n",
    "This is the most important concept!\n",
    "\n",
    "ğŸ’¡Â **What is self-attention?\n",
    "**It allows the transformer toÂ **focus on different words in a sentence depending on context.**\n",
    "\n",
    "**Example:**Consider two sentences:\n",
    "1ï¸âƒ£Â **\"I went to the bank to deposit money.\"**\n",
    "2ï¸âƒ£Â **\"I sat by the bank of the river.\"**\n",
    "\n",
    "The wordÂ **\"bank\"**Â has different meanings in each sentence.\n",
    "Self-attention helps the modelÂ **look at surrounding words**Â to understand the meaning of \"bank\" correctly.\n",
    "\n",
    "#### **How Self-Attention Works (Conceptually)**\n",
    "\n",
    "*   Each word getsÂ **three vectors**:\n",
    "    \n",
    "    *   **Query (Q)**Â â€“ What is this word looking for?\n",
    "        \n",
    "    *   **Key (K)**Â â€“ How important is this word for others?\n",
    "        \n",
    "    *   **Value (V)**Â â€“ What information does this word carry?\n",
    "        \n",
    "*   The modelÂ **compares every word to every other word**Â and assignsÂ **attention scores**.\n",
    "    \n",
    "*   Words withÂ **higher scores**Â are more relevant.\n",
    "    \n",
    "\n",
    "ğŸ”¹Â **Example (Attention Scores for \"bank\")**\n",
    "\n",
    "| Word    | \"bank\" in Sentence 1 | \"bank\" in Sentence 2 |\n",
    "|---------|----------------------|----------------------|\n",
    "| I       | 0.1                  | 0.1                  |\n",
    "| went    | 0.2                  | 0.2                  |\n",
    "| to      | 0.3                  | 0.3                  |\n",
    "| the     | 0.5                  | 0.5                  |\n",
    "| bank    | 1.0                  | 1.0                  |\n",
    "| deposit | 0.9                  | 0.1                  |\n",
    "| river   | 0.1                  | 0.9                  |\n",
    "\n",
    "Here,Â **\"deposit\" has a high score in Sentence 1**, meaning \"bank\" refers to a financial institution.In Sentence 2,Â **\"river\" has a high score**, so \"bank\" refers to a riverbank.\n",
    "\n",
    "This is how transformersÂ **understand context better than RNNs**.\n",
    "\n",
    "### **4ï¸âƒ£ Multi-Head Attention (Learning Multiple Contexts)**\n",
    "\n",
    "Instead of looking at justÂ **one relationship at a time**, transformers useÂ **multiple attention heads**Â to understandÂ **different aspects of meaning simultaneously**.\n",
    "\n",
    "For example, inÂ **machine translation**:\n",
    "\n",
    "*   **One head**Â might focus on grammar.\n",
    "    \n",
    "*   **Another head**Â might focus on word meaning.\n",
    "    \n",
    "*   **Another head**Â might focus on sentence structure.\n",
    "    \n",
    "\n",
    "This makes transformersÂ **powerful and highly context-aware**.\n",
    "\n",
    "### **5ï¸âƒ£ Feed-Forward Neural Network**\n",
    "\n",
    "After self-attention, each word passes through aÂ **feed-forward neural network**Â that refines the understanding.These networks helpÂ **add more complex reasoning**Â before making predictions.\n",
    "\n",
    "### **6ï¸âƒ£ Layer Normalization & Residual Connections**\n",
    "\n",
    "To make training stable and efficient, transformers use: \n",
    "\n",
    "âœ…Â **Residual connections**Â â€“ Prevent information loss.\n",
    "\n",
    "âœ…Â **Layer normalization**Â â€“ Ensure numerical stability.\n",
    "\n",
    "**ğŸ”¹ Example: How Transformers Work in Text Generation**\n",
    "--------------------------------------------------------\n",
    "\n",
    "Letâ€™s say we train a transformer to predict the next word in a sentence.\n",
    "\n",
    "Input:ğŸ‘‰Â **\"The cat sat on the\"**Possible outputs:\n",
    "\n",
    "ğŸ”¹Â \"mat\"Â (80% confidence)\n",
    "\n",
    "ğŸ”¹Â \"sofa\"Â (10% confidence)\n",
    "\n",
    "ğŸ”¹Â \"chair\"Â (5% confidence)\n",
    "\n",
    "ğŸ”¹Â \"floor\"Â (5% confidence)\n",
    "\n",
    "The modelÂ **assigns probabilities**Â to each possible next word and picks the most likely one.\n",
    "\n",
    "**ğŸ”¹ Comparison: Transformers vs. RNNs/LSTMs**\n",
    "----------------------------------------------\n",
    "\n",
    "| Feature              | RNN/LSTM                  | Transformer              |\n",
    "|----------------------|--------------------------|--------------------------|\n",
    "| **Processing**       | Sequential (word-by-word) | Parallel (Fast)          |\n",
    "| **Long-Term Memory** | Struggles with long texts | Handles long-range dependencies |\n",
    "| **Training Speed**   | Slow                      | Fast (processes entire text at once) |\n",
    "| **Context Understanding** | Limited             | Strong (Self-Attention)  |\n",
    "| **Scalability**      | Hard to scale             | Easily scales with GPUs  |\n",
    "\n",
    "\n",
    "**ğŸ”¹ Real-World Applications of Transformers**\n",
    "----------------------------------------------\n",
    "\n",
    "ğŸš€Â **GPT (ChatGPT, GPT-4)**Â â€“ Conversational AI.\n",
    "\n",
    "ğŸš€Â **BERT**Â â€“ Google Search, sentiment analysis.\n",
    "\n",
    "ğŸš€Â **T5 (Text-to-Text Transfer Transformer)**Â â€“ Summarization, translation.\n",
    "\n",
    "ğŸš€Â **DALLÂ·E**Â â€“ Image generation from text prompts.\n",
    "\n",
    "ğŸš€Â **Whisper**Â â€“ Automatic speech recognition (ASR).\n",
    "\n",
    "**ğŸ”¹ Comparison: GPT and BERT**\n",
    "----------------------------------------------\n",
    "\n",
    "| Feature                      | Generative Pre-trained Transformer (GPT) | Bidirectional Encoder Representations from Transformers (BERT) |\n",
    "|------------------------------|------------------------------------------|---------------------------------------------------------------|\n",
    "| **Architecture**             | Decoder-only Transformer                 | Encoder-only Transformer                                      |\n",
    "| **Training Approach**        | Autoregressive (predicts next token)     | Autoencoding (masked language model)                          |\n",
    "| **Directionality**           | Unidirectional (left-to-right)           | Bidirectional (both left and right context)                   |\n",
    "| **Primary Use Case**         | Text generation, dialogue, content creation | Text understanding, classification, sentiment analysis       |\n",
    "| **Example Tasks**            | Chatbots, story writing, code generation  | Q&A, Named Entity Recognition, Sentiment Analysis             |\n",
    "| **Input Type**               | Processes input sequentially (causal)    | Processes entire input simultaneously                         |\n",
    "| **Pretraining Objective**    | Next-word prediction                     | Masked word prediction + Next sentence prediction             |\n",
    "| **Context Awareness**        | Limited by past tokens only              | Full sentence-level understanding                             |\n",
    "| **Computational Cost**       | High (due to sequential generation)      | Lower (processes tokens in parallel)                          |\n",
    "| **Fine-tuning Flexibility**  | Good for open-ended text generation      | Better for structured NLP tasks                              |\n",
    "| **Strengths**                | Generates coherent, fluent text          | Strong at understanding sentence semantics                   |\n",
    "| **Weaknesses**               | May hallucinate or generate incorrect info | Not good at free-text generation                             |\n",
    "| **Famous Models**            | GPT-3, GPT-4, ChatGPT                    | BERT, RoBERTa, DistilBERT                                    |\n",
    "| **Use in Industry**          | AI assistants, content creation, coding  | Search engines, customer support, text classification        |\n",
    "\n",
    "\n",
    "**ğŸ”¹ Summary**\n",
    "--------------\n",
    "\n",
    "âœ… Transformers useÂ **self-attention**Â to understand context.\n",
    "\n",
    "âœ… TheyÂ **process entire text in parallel**, making themÂ **fast and scalable**.\n",
    "\n",
    "âœ… Used inÂ **chatbots, search engines, translation, and AI text generation**.\n",
    "\n",
    "âœ…Â **Outperform RNNs and LSTMs**Â in most NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-30 14:02:31.997657: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' floor', ' bed', ' couch', ' ground', ' edge']\n",
      "[' a', ' the', ' an', ' not', ' one']\n",
      "[' the', ' our', ' how', ' their', ' everything']\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel  # For loading the GPT-2 model and tokenizer\n",
    "import torch  # For tensor operations and model inference\n",
    "\n",
    "# -------------------------------\n",
    "# Load Pre-trained GPT-2 Model and Tokenizer\n",
    "# -------------------------------\n",
    "\n",
    "# Load the pre-trained GPT-2 tokenizer\n",
    "# The tokenizer converts input text into token IDs that the model can process.\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Load the pre-trained GPT-2 language model\n",
    "# The model generates text by predicting the next word based on the input context.\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model.eval()  # Set the model to evaluation mode (disables dropout, etc.)\n",
    "\n",
    "# -------------------------------\n",
    "# Prediction Function\n",
    "# -------------------------------\n",
    "\n",
    "def predict_next_word(text, top_k=5):\n",
    "    if not text.strip():\n",
    "        raise ValueError(\"Input text cannot be empty\")\n",
    "    \"\"\"\n",
    "    Predicts the next word(s) for a given input text using the GPT-2 model.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text for which the next word(s) are predicted.\n",
    "        top_k (int): The number of top probable next words to return (default is 5).\n",
    "\n",
    "    Returns:\n",
    "        list of str: A list of the top `top_k` predicted next words.\n",
    "\n",
    "    Steps:\n",
    "    1. Tokenize the input text using the GPT-2 tokenizer.\n",
    "    2. Pass the tokenized input to the GPT-2 model to get the logits (raw predictions).\n",
    "    3. Extract the logits for the last token in the sequence.\n",
    "    4. Apply the softmax function to convert logits into probabilities.\n",
    "    5. Use `torch.topk` to get the top `top_k` tokens with the highest probabilities.\n",
    "    6. Decode the token IDs back into words using the tokenizer.\n",
    "\n",
    "    Example:\n",
    "        Input: \"The cat sat on the\"\n",
    "        Output: [\"mat\", \"floor\", \"sofa\", \"chair\", \"bed\"]\n",
    "    \"\"\"\n",
    "    # Tokenize the input text and convert it into tensors\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    \n",
    "    # Perform inference with the GPT-2 model (no gradient computation needed)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Extract the logits for the last token in the sequence\n",
    "    logits = outputs.logits[:, -1, :]\n",
    "    \n",
    "    # Apply softmax to convert logits into probabilities\n",
    "    probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    \n",
    "    # Get the top `top_k` tokens with the highest probabilities\n",
    "    top_k_tokens = torch.topk(probabilities, top_k)\n",
    "    \n",
    "    # Decode the token IDs back into words\n",
    "    next_words = [tokenizer.decode([token]) for token in top_k_tokens.indices[0]]\n",
    "    \n",
    "    return next_words\n",
    "\n",
    "# -------------------------------\n",
    "# Example Predictions\n",
    "# -------------------------------\n",
    "\n",
    "# Predict the next word(s) for various input texts\n",
    "# The predictions are based on the context provided in the input text.\n",
    "print(predict_next_word(\"The cat sat on the\"))  # Example output: [\"mat\", \"floor\", \"sofa\", \"chair\", \"bed\"]\n",
    "print(predict_next_word(\"Deep learning is\"))  # Example output: [\"transforming\", \"revolutionizing\", \"advancing\", \"changing\", \"reshaping\"]\n",
    "print(predict_next_word(\"Transformers are revolutionizing\"))  # Example output: [\"AI\", \"technology\", \"NLP\", \"research\", \"science\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
